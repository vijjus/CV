{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Xception.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e1d139d9b1147ceb9fa639ba54db215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3ebc0983827f4f888b342bfbcf4cb533",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fbaa0a18feb74031993ec7450e110988",
              "IPY_MODEL_eb4c6319c40f45638ddfb897cda68b30"
            ]
          }
        },
        "3ebc0983827f4f888b342bfbcf4cb533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbaa0a18feb74031993ec7450e110988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_614046b1b82d4df0bfea76b111fcaa85",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 91675053,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 91675053,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a19dc7984afb4006aabeba8633c86f2d"
          }
        },
        "eb4c6319c40f45638ddfb897cda68b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_77f46d79592e4811b45f380f05f1550b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 87.4M/87.4M [07:39&lt;00:00, 199kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02729c06407143068cd06ff7775f3627"
          }
        },
        "614046b1b82d4df0bfea76b111fcaa85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a19dc7984afb4006aabeba8633c86f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77f46d79592e4811b45f380f05f1550b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02729c06407143068cd06ff7775f3627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e609d0a420342938dc05f986bb501e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8659412f52d041fa8790a1f1a19d3ce7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e0d47d3df6334ccf8d0d3c71d7c837b0",
              "IPY_MODEL_e010173e9f674d929b0278349e91d6e2"
            ]
          }
        },
        "8659412f52d041fa8790a1f1a19d3ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0d47d3df6334ccf8d0d3c71d7c837b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c4c1a51941df4346b238ef8d00f16266",
            "_dom_classes": [],
            "description": " 61%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3370,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2058,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fdfa2d9758942fd8c335855f5995f4c"
          }
        },
        "e010173e9f674d929b0278349e91d6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0fa02d1c5ad4d07b2b823d9b631ff6b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2058/3370 [1:41:15&lt;53:12,  2.43s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_401d5ece76a640a8ba253b25c67d52ed"
          }
        },
        "c4c1a51941df4346b238ef8d00f16266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fdfa2d9758942fd8c335855f5995f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0fa02d1c5ad4d07b2b823d9b631ff6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "401d5ece76a640a8ba253b25c67d52ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijjus/CV/blob/master/Xception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV3L5wdWAaYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "5c7aa157-23f0-427b-a9d7-9338f8bf303f"
      },
      "source": [
        "pip install pretrainedmodels"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.6.1+cu101)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.12.0)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=4d64b56402c98675ce79dddfab97768f7dd76bb1955cf360a1379c7d79bba260\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3PTd_rSoH65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "#import timm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.ops as ops\n",
        "import cv2\n",
        "from collections import OrderedDict\n",
        "\n",
        "plt.ion()   # interactive mode "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fqTjgKeo1s-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "554c140a-4512-4f94-ef89-2f19b31137c5"
      },
      "source": [
        "!echo '3FDsALdx4sLyMedGxO5h:RsdZX1E2jRryINc6Vb5HGpL1FY84lNBXbTNSU8Ln' > ~/.passwd-s3fs\n",
        "!cat ~/.passwd-s3fs\n",
        "!chmod 600 ~/.passwd-s3fs\n",
        "!mkdir /s3\n",
        "!apt install s3fs\n",
        "!s3fs w251-final-project /s3 -o passwd_file=~/.passwd-s3fs,url=https://s3.us.cloud-object-storage.appdomain.cloud"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3FDsALdx4sLyMedGxO5h:RsdZX1E2jRryINc6Vb5HGpL1FY84lNBXbTNSU8Ln\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  s3fs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 59 not upgraded.\n",
            "Need to get 200 kB of archives.\n",
            "After this operation, 557 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 s3fs amd64 1.82-1 [200 kB]\n",
            "Fetched 200 kB in 0s (2,973 kB/s)\n",
            "Selecting previously unselected package s3fs.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/s3fs_1.82-1_amd64.deb ...\n",
            "Unpacking s3fs (1.82-1) ...\n",
            "Setting up s3fs (1.82-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bfknbnS-oxo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8bef6c8c-2f8c-414d-8447-30a492296c93"
      },
      "source": [
        "!ls /s3/DFDC"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataloader\t   dfdc_train_part_1  dfdc_train_part_3  dfdc_train_part_5\n",
            "dfdc_train_part_0  dfdc_train_part_2  dfdc_train_part_4  models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chk-m-UouV7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"xception\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 15\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAAtGtTSANOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1eb2b42b-30f6-4d6d-c9f5-91b9f631be1d"
      },
      "source": [
        "import pretrainedmodels\n",
        "print(pretrainedmodels.model_names)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetamobile', 'nasnetalarge', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107', 'xception', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'pnasnet5large', 'polynet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaC0Wv8RmTHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "060a0d87-07e7-4976-f377-753d33fe074c"
      },
      "source": [
        "print(pretrainedmodels.pretrained_settings[model_name])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth', 'input_space': 'RGB', 'input_size': [3, 299, 299], 'input_range': [0, 1], 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'num_classes': 1000, 'scale': 0.8975}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8BEhrdjASgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "5e1d139d9b1147ceb9fa639ba54db215",
            "3ebc0983827f4f888b342bfbcf4cb533",
            "fbaa0a18feb74031993ec7450e110988",
            "eb4c6319c40f45638ddfb897cda68b30",
            "614046b1b82d4df0bfea76b111fcaa85",
            "a19dc7984afb4006aabeba8633c86f2d",
            "77f46d79592e4811b45f380f05f1550b",
            "02729c06407143068cd06ff7775f3627"
          ]
        },
        "outputId": "703f852a-a330-4903-cbea-b834d5bd366f"
      },
      "source": [
        "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
        "#model.eval()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth\" to /root/.cache/torch/checkpoints/xception-43020ad28.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e1d139d9b1147ceb9fa639ba54db215",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=91675053.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3gR8iUs3Thl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################\n",
        "# Load Data\n",
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(299),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        #transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        #Noise(0.25, 0.05)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(299),\n",
        "        #transforms.CenterCrop(229),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEHRWpGJ3HgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset that includes image file paths. Extends\n",
        "    torchvision.datasets.ImageFolder\n",
        "    \"\"\"\n",
        "\n",
        "    # override the __getitem__ method. this is the method that dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "    # this is what ImageFolder normally returns \n",
        "        try:\n",
        "            original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "            # the image file path\n",
        "            path = self.imgs[index][0]\n",
        "\n",
        "            # make a new tuple that includes original and the path\n",
        "            tuple_with_path = (original_tuple + (path,))\n",
        "\n",
        "            return tuple_with_path\n",
        "        \n",
        "        except Exception as e:\n",
        "            return None"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FYNfaAE7fAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_collate(batch): # batch size 4 [{tensor image, tensor label},{},{},{}] could return something like G = [None, {},{},{}]\n",
        "    batch = list(filter (lambda x:x is not None, batch)) # this gets rid of nones in batch. For example above it would result to G = [{},{},{}]\n",
        "    # I want len(G) = 4\n",
        "    # so how to sample another dataset entry?\n",
        "    return torch.utils.data.dataloader.default_collate(batch) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAbs66tO6Zwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch.utils.data\n",
        "import random\n",
        "import torch\n",
        "is_torchvision_installed = True\n",
        "try:\n",
        "    import torchvision\n",
        "except:\n",
        "    is_torchvision_installed = False\n",
        "class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, dataset, labels=None):\n",
        "        self.labels = labels\n",
        "        self.dataset = dict()\n",
        "        self.balanced_max = 0\n",
        "        # Save all the indices for all the classes\n",
        "        for idx in range(0, len(dataset)):\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label not in self.dataset:\n",
        "                self.dataset[label] = list()\n",
        "            self.dataset[label].append(idx)\n",
        "            self.balanced_max = len(self.dataset[label]) \\\n",
        "                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n",
        "        \n",
        "        # Oversample the classes with fewer elements than the max\n",
        "        for label in self.dataset:\n",
        "            while len(self.dataset[label]) < self.balanced_max:\n",
        "                self.dataset[label].append(random.choice(self.dataset[label]))\n",
        "        self.keys = list(self.dataset.keys())\n",
        "        self.currentkey = 0\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "\n",
        "    def __iter__(self):\n",
        "        while self.indices[self.currentkey] < self.balanced_max - 1:\n",
        "            self.indices[self.currentkey] += 1\n",
        "            yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]\n",
        "            self.currentkey = (self.currentkey + 1) % len(self.keys)\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "    \n",
        "    def _get_label(self, dataset, idx, labels = None):\n",
        "        if self.labels is not None:\n",
        "            return self.labels[idx].item()\n",
        "        else:\n",
        "            # Trying guessing\n",
        "            dataset_type = type(dataset)\n",
        "            if is_torchvision_installed and dataset_type is torchvision.datasets.MNIST:\n",
        "                return dataset.train_labels[idx].item()\n",
        "            elif is_torchvision_installed and dataset_type is torchvision.datasets.ImageFolder:\n",
        "                return dataset.imgs[idx][1]\n",
        "            elif is_torchvision_installed and dataset_type.__bases__[0] is torchvision.datasets.folder.ImageFolder:\n",
        "                return dataset.imgs[idx][1]\n",
        "            else:\n",
        "                raise Exception(\"You should pass the tensor of labels to the constructor as second argument\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.balanced_max*len(self.keys)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5EIGv3ZvCk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b9d01dfc-5701-4996-e767-cf61112f2696"
      },
      "source": [
        "%%time\n",
        "\n",
        "data_dir = '/s3/DFDC/dataloader/'\n",
        "\n",
        "image_datasets = {x: ImageFolderWithPaths(os.path.join(data_dir, x), data_transforms[x])                    \n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "# balanced sampler dataloader\n",
        "dataloaders = {x: torch.utils.data.DataLoader(\n",
        "    image_datasets[x], sampler=BalancedBatchSampler(image_datasets[x], None), \n",
        "    batch_size=16, shuffle=False, num_workers=2, collate_fn=my_collate)\n",
        "               for x in ['train', 'val']}\n",
        "\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "class_names = image_datasets['train'].classes"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 210 ms, sys: 13.8 ms, total: 224 ms\n",
            "Wall time: 5min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luuwcgo08tld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "88da251c-ef64-44e0-c4d2-b47ff3f37048"
      },
      "source": [
        "image_datasets"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': Dataset ImageFolderWithPaths\n",
              "     Number of datapoints: 29054\n",
              "     Root location: /s3/DFDC/dataloader/train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                RandomResizedCrop(size=(299, 299), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
              "                RandomHorizontalFlip(p=0.5)\n",
              "                ToTensor()\n",
              "                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "            ), 'val': Dataset ImageFolderWithPaths\n",
              "     Number of datapoints: 16421\n",
              "     Root location: /s3/DFDC/dataloader/val\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
              "                CenterCrop(size=(229, 229))\n",
              "                ToTensor()\n",
              "                Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "            )}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlVPyuLi9m8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4def5974-d2a8-43a8-c16e-8f52a302bd0a"
      },
      "source": [
        "class_names"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['FAKE', 'REAL']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rjKk8hZmzPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize images\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVjO5-UTmzPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randrange"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpI_qFb7mzPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sample(s):\n",
        "    ds = image_datasets[s]\n",
        "    sz = len(ds)\n",
        "    n = randrange(20)\n",
        "    print(sz,n)\n",
        "    i = 0\n",
        "    for data in dataloaders[s]:\n",
        "        i += 1\n",
        "        if i == n:\n",
        "            X, y, z = data\n",
        "            return X, y, z"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK5dF5VPyfWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyNet(torch.nn.Module):\n",
        "  def __init__(self, model_name):\n",
        "    super(MyNet, self).__init__()\n",
        "    self.net = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
        "    self.fc = nn.Linear(self.net.last_linear.out_features, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.net(x)\n",
        "    return self.fc(x)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ppChzvcz227",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = MyNet(model_name)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3_7Dr6WmzPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X, y, z = get_sample('train')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwwsllM4-Ylf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_image(X):\n",
        "    img = X/X.max().item()\n",
        "    img = (img + 1)/2.0\n",
        "    plt.imshow(img.permute(1,2,0).numpy())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5X85G6s8CLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcSjApPVumvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to train model\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            dataset_size = 0\n",
        "            \n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels, paths in tqdm.notebook.tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                dataset_size += len(paths)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                        \n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size         \n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            print(dataset_size)\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-ZYXgVD_Pku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize predictions\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels, paths) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "#             paths = paths.to(device)\n",
        "            # asynchronous\n",
        "#             inputs = inputs.to(device, non_blocking=True)\n",
        "#             labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdAH8IDgmzQa",
        "colab_type": "text"
      },
      "source": [
        "## Train with seed 1337"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJce9QlWmzQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.backends.cudnn.deterministic = True\n",
        "#torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3K62kTRmzQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydYE_CvTmzQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "set_seed_everywhere(1337, device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd9WgRwMmzQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model onto each GPU\n",
        "net = net.to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqX4hh45mzQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# asynchronous\n",
        "# model_ft = model_ft.to(device, non_blocking=True) \n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# # 0.1 decay rate every 7 steps\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47NDvcuBVNYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KlrEMsZ3mzQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "6e609d0a420342938dc05f986bb501e3",
            "8659412f52d041fa8790a1f1a19d3ce7",
            "e0d47d3df6334ccf8d0d3c71d7c837b0",
            "e010173e9f674d929b0278349e91d6e2",
            "c4c1a51941df4346b238ef8d00f16266",
            "2fdfa2d9758942fd8c335855f5995f4c",
            "a0fa02d1c5ad4d07b2b823d9b631ff6b",
            "401d5ece76a640a8ba253b25c67d52ed"
          ]
        },
        "outputId": "dab8458e-b2ab-4349-fe2e-65285d99b773"
      },
      "source": [
        "# train and evaluate model\n",
        "model_ft = train_model(net, criterion, optimizer_ft,\n",
        "                         exp_lr_scheduler, num_epochs=10)\n",
        "\n",
        "visualize_model(model_ft)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e609d0a420342938dc05f986bb501e3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3370.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}