{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IAMwords_CRNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijjus/CV/blob/master/IAMwords_CRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liPEzVvXv1zB"
      },
      "source": [
        "load_weights = False\n",
        "do_data_analysis = False\n",
        "IMG_SIZE=(32, 128)\n",
        "weigths_file = \"/content/gdrive/MyDrive/EffNetRNN_IAMWords.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi6Kay_xUiBy"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYJeA9q8Uv0J"
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1K8SQqsVJIs"
      },
      "source": [
        "!pip install colorama"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6tOz1T7gYoE"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import sys\n",
        "import torch\n",
        "import collections\n",
        "import editdistance\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from colorama import Fore\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from itertools import groupby\n",
        "import albumentations as A\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from albumentations.pytorch import ToTensor\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable, Dict, List, Set\n",
        "from torch.nn import Conv2d, Dropout, LogSoftmax\n",
        "from torch.utils.data.dataloader import default_collate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWD1Wp3jgsPD"
      },
      "source": [
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFwSIq-lgvVV"
      },
      "source": [
        "!cp /content/gdrive/MyDrive/words.tgz ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9DFwk0Ghbci"
      },
      "source": [
        "!mkdir data; mv words.tgz data/ ; cd data ; tar -zxf words.tgz; ls ; cd -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gzt4O2LaaBJ"
      },
      "source": [
        "!rm ./data/a01/a01-117/a01-117-05-02.png\n",
        "!rm ./data/r06/r06-022/r06-022-03-05.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZQ7uhTnh77-"
      },
      "source": [
        "img = cv2.imread('data/a01/a01-000u/a01-000u-05-00.png', cv2.IMREAD_GRAYSCALE)\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqRs2iyaiuVy"
      },
      "source": [
        "!cp /content/gdrive/MyDrive/ascii.tgz ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjYZfZRNjNWQ"
      },
      "source": [
        "!tar -zxf ascii.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwJ8aPVjjqJ5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqoXdUL9jrRb"
      },
      "source": [
        "!tail -10 words.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tm4M-HIjutd"
      },
      "source": [
        "def list_files_and_basename_recursive(dirpath, ext='.png'):\n",
        "    # TODO unit test\n",
        "    retlist = {}\n",
        "    for root, dirs, files in os.walk(dirpath):\n",
        "        for filename in files:\n",
        "            if not filename.endswith(ext):\n",
        "                continue\n",
        "\n",
        "            image_filename_no_ext = os.path.basename(os.path.splitext(filename)[0])\n",
        "            retlist[image_filename_no_ext] = os.path.join(root, filename)\n",
        "\n",
        "    return retlist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1OaqAz_lvdg"
      },
      "source": [
        "all_files = list_files_and_basename_recursive('./data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj3kz9Fsl-4k"
      },
      "source": [
        "all_keys = list(all_files.keys())\n",
        "num_total = len(all_keys)\n",
        "print(f'There are {num_total} words in all..')\n",
        "shuffle(all_keys)\n",
        "num_train = int(0.8 * num_total)\n",
        "train_words = all_keys[:num_train]\n",
        "test_words = all_keys[num_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veItSPV_o1yX"
      },
      "source": [
        "WORD_LEVEL_FIELDNAMES = [\n",
        "        \"word_id\",\n",
        "        \"word_segmentation_result\",\n",
        "        \"graylevel\",\n",
        "        \"bbox_x\",\n",
        "        \"bbox_y\",\n",
        "        \"bbox_w\",\n",
        "        \"bbox_h\",\n",
        "        \"tag\",\n",
        "        \"label\",\n",
        "    ]\n",
        "all_labels = dict()\n",
        "with open('words.txt', 'r') as fp:\n",
        "  # QUOTE_NONE is required here, there are some open double quotes in the file that screw with parsing\n",
        "  reader = csv.DictReader(\n",
        "      fp,\n",
        "      fieldnames=WORD_LEVEL_FIELDNAMES,\n",
        "      delimiter=\" \",\n",
        "      quoting=csv.QUOTE_NONE,\n",
        "  )\n",
        "  # skip the first 18 lines\n",
        "  for i in range(18):\n",
        "    next(reader)\n",
        "  for row in reader:\n",
        "    all_labels[row['word_id']] = row['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhh2UWPgnX5I"
      },
      "source": [
        "def write_meta(file_name, word_list):\n",
        "  with open(file_name, 'w') as fp:\n",
        "    count = 0\n",
        "    for entry in word_list:\n",
        "      fp.write(f'{all_labels[entry]} {all_files[entry]}'+'\\n')\n",
        "      count += 1\n",
        "    print(f'Wrote {count} entries to {file_name}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe0rxkYCoVxp"
      },
      "source": [
        "write_meta('train.txt', train_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cti7xG8KolWJ"
      },
      "source": [
        "write_meta('test.txt', test_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z95leiFowIk"
      },
      "source": [
        "!tail -10 train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syWxMqUw4EGL"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Convert between str and label.\n",
        "    NOTE:\n",
        "        Insert `blank` to the alphabet for CTC.\n",
        "    Args:\n",
        "        alphabet (Iterable): set of the possible characters.\n",
        "        ignore_case (bool, default=False): whether or not to ignore all of the case.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alphabet: List):\n",
        "        self.alphabet = alphabet\n",
        "\n",
        "        self.dict = {}\n",
        "        for i, char in enumerate(alphabet):\n",
        "            self.dict[char] = i\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self.dict)\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Support batch or single str.\n",
        "        Args:\n",
        "            text (str or list of str): texts to convert.\n",
        "        Returns:\n",
        "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
        "            torch.IntTensor [n]: length of each text.\n",
        "        \"\"\"\n",
        "        if isinstance(text, str):\n",
        "            text = [self.dict[char] for char in text]\n",
        "            length = [len(text)]\n",
        "        elif isinstance(text, torch.Tensor):\n",
        "            if text.dim() == 2:\n",
        "                label = []\n",
        "                length = []\n",
        "                for sub in text:\n",
        "                    label_char = [self.dict[str(x)] for x in sub.tolist()]\n",
        "                    label.append(label_char)\n",
        "                    length.append(len(sub))\n",
        "                text = label\n",
        "            else:\n",
        "                text = [self.dict[str(x)] for x in text.tolist()]\n",
        "                length = [len(text)]\n",
        "        elif isinstance(text, collections.Iterable):\n",
        "            length = [len(s) for s in text]\n",
        "            text = \"\".join(text)\n",
        "            text, _ = self.encode(text)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid input type {type(text)}\")\n",
        "\n",
        "        return torch.IntTensor(text), torch.IntTensor(length)\n",
        "\n",
        "    def decode(self, t, length, raw=False):\n",
        "        \"\"\"Decode encoded texts back into strs.\n",
        "        Args:\n",
        "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
        "            torch.IntTensor [n]: length of each text.\n",
        "        Raises:\n",
        "            AssertionError: when the texts and its length does not match.\n",
        "        Returns:\n",
        "            text (str or list of str): texts to convert.\n",
        "        \"\"\"\n",
        "        if length.numel() == 1:\n",
        "            length = length[0]\n",
        "            assert (\n",
        "                t.numel() == length\n",
        "            ), \"text with length: {} does not match declared length: {}\".format(\n",
        "                t.numel(), length\n",
        "            )\n",
        "            if raw:\n",
        "                return \"\".join(\n",
        "                    [self.alphabet[i] if self.alphabet[i] != \"<SEP>\" else \"\" for i in t]\n",
        "                )\n",
        "            else:\n",
        "                char_list = []\n",
        "                for i in range(length):\n",
        "                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n",
        "                        c = self.alphabet[t[i]]\n",
        "                        if c == \"<SEP>\":\n",
        "                            c = \" \"\n",
        "                        char_list.append(c)\n",
        "                return \"\".join(char_list)\n",
        "        else:\n",
        "            # batch mode\n",
        "            assert (\n",
        "                t.numel() == length.sum()\n",
        "            ), \"texts with length: {} does not match declared length: {}\".format(\n",
        "                t.numel(), length.sum()\n",
        "            )\n",
        "            texts = []\n",
        "            index = 0\n",
        "            for i in range(length.numel()):\n",
        "                label_length = length[i]\n",
        "                texts.append(\n",
        "                    self.decode(\n",
        "                        t[index : index + label_length],\n",
        "                        torch.IntTensor([label_length]),\n",
        "                        raw=raw,\n",
        "                    )\n",
        "                )\n",
        "                index += label_length\n",
        "            return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHheatlCWK9V"
      },
      "source": [
        "def base_collator_with_padding(\n",
        "    pad_key, base_collator_fn=default_collate, pad_val=0, pad_idx=0\n",
        "):\n",
        "    \"\"\"Pad: a given key in the data iterator output to a consistent length.\n",
        "    Otherwise, falls back to default collate functionality\n",
        "    Args:\n",
        "        pad_key: str The specific data stream to pad\n",
        "        base_collator_fn: Callable The default collator to call. Defaults to default_collate from PyTorch\n",
        "        pad_val: Any The value to pad in the tensor. Defaults to 0\n",
        "    Returns:\n",
        "        The result of collation, including padding of the variable length streams\n",
        "    \"\"\"\n",
        "\n",
        "    def fn(batch):\n",
        "        max_len = max(item[pad_key].shape[pad_idx] for item in batch)\n",
        "\n",
        "        for item in batch:\n",
        "            original_tensor = item[pad_key]\n",
        "\n",
        "            # TODO currently assumes a single dimension tensor\n",
        "            padded_tensor = torch.full((max_len,), pad_val, dtype=original_tensor.dtype)\n",
        "            padded_tensor[: original_tensor.shape[pad_idx]] = original_tensor\n",
        "\n",
        "            item[pad_key] = padded_tensor\n",
        "\n",
        "        return base_collator_fn(batch)\n",
        "\n",
        "    return fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnT8mP83hOZr"
      },
      "source": [
        "def resize_image(image, img_size=IMG_SIZE):\n",
        "    if isinstance(image, str):\n",
        "      image = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "    scale_coef = float(img_size[0]) / image.shape[0]\n",
        "    new_width = int(image.shape[1] * scale_coef)\n",
        "\n",
        "    if new_width > img_size[1]:\n",
        "        # TODO this is a hack\n",
        "        resized_image = cv2.resize(\n",
        "            image, (img_size[1], img_size[0]), cv2.INTER_AREA\n",
        "        )\n",
        "\n",
        "        resized_image = resized_image\n",
        "        return np.reshape(resized_image, img_size)\n",
        "\n",
        "    new_height = img_size[0]\n",
        "    resized_image = cv2.resize(image, (new_width, new_height), cv2.INTER_CUBIC)\n",
        "\n",
        "    padded_image = np.full(img_size, 255, dtype=image.dtype)\n",
        "\n",
        "    padded_image[\n",
        "        : resized_image.shape[0], : resized_image.shape[1]\n",
        "    ] = resized_image\n",
        "\n",
        "    padded_image = padded_image\n",
        "    padded_image = np.reshape(padded_image, img_size)\n",
        "    return padded_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_YowOUIrEtx"
      },
      "source": [
        "class IAMDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        words_file: str,\n",
        "        transform: Callable = None,\n",
        "        max_sequence_len=None,\n",
        "    ):\n",
        "        self._data = []\n",
        "        self._transform = transform\n",
        "\n",
        "        chars: Set[str] = set()\n",
        "\n",
        "        self._limit_max_sequence_length = bool(max_sequence_len)\n",
        "\n",
        "        # either set it to our upper limit, or initialize it to be overridden by our longest sequence\n",
        "        self._max_sequence_length = (\n",
        "            max_sequence_len if self._limit_max_sequence_length else -1\n",
        "        )\n",
        "\n",
        "        with open(words_file) as fp:\n",
        "            for line in fp:\n",
        "                entries = line.split()\n",
        "                label = entries[0]\n",
        "                image_filepath = entries[1]\n",
        "                data_dict = {\n",
        "                    \"image_filepath\": image_filepath,\n",
        "                }\n",
        "\n",
        "                label_len = len(label)\n",
        "                label_str = label[: self._max_sequence_length]\n",
        "                if self._limit_max_sequence_length:\n",
        "                    label_str = label_str[: self._max_sequence_length]\n",
        "                    new_label_len = len(label_str)\n",
        "                    if label_len != new_label_len:\n",
        "                        warnings.warn(\n",
        "                            f\"Had to truncate image label {row['word_id']}, label string too long \"\n",
        "                            f\"(was {label_len}, limit {self._max_sequence_length})\"\n",
        "                        )\n",
        "\n",
        "                elif label_len > self._max_sequence_length:\n",
        "                    self._max_sequence_length = label_len\n",
        "\n",
        "                chars.update(label_str)\n",
        "\n",
        "                data_dict[\"label\"] = label_str\n",
        "                self._data.append(data_dict)\n",
        "\n",
        "        # Could be an issue if the train set doesn't have some symbols.\n",
        "        # NOTE index 0 is our blank space index\n",
        "        if \" \" in chars:\n",
        "            chars.remove(\" \")\n",
        "\n",
        "        self.vocab = Vocabulary([\"<SEP>\"] + sorted(chars))\n",
        "\n",
        "    # TODO num_classes and max_seq_len should be precalculated\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return self.vocab.num_classes\n",
        "\n",
        "    @property\n",
        "    def max_sequence_len(self):\n",
        "        return self._max_sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_at_idx = self._data[idx]\n",
        "        image_filepath = data_at_idx[\"image_filepath\"]\n",
        "        image_label = data_at_idx[\"label\"]\n",
        "\n",
        "        image = cv2.imread(image_filepath, cv2.IMREAD_GRAYSCALE)\n",
        "        image = resize_image(image)\n",
        "\n",
        "        if image is None:\n",
        "          print(f'Trouble reading {image_filepath}')\n",
        "\n",
        "        # invert the image\n",
        "        image = image.max() - image\n",
        "        if self._transform:\n",
        "            image = self._transform(image=image)['image']\n",
        "            image = image.permute(1, 0)\n",
        "\n",
        "        label_indices, label_length = self.vocab.encode(image_label)\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"label_indices\": label_indices,\n",
        "            \"label_length\": label_length,\n",
        "            \"label_str\": image_label,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtvxDK84RpaT"
      },
      "source": [
        "def statX(x):\n",
        "  print(f\"max={torch.max(x)} min={torch.min(x)}, mean={torch.mean(x):.2f}, var={torch.var(x):.2f}, std={torch.std(x):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zod-FT2tWcQB"
      },
      "source": [
        "train_transform = A.Compose([\n",
        "    #A.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=3, p=0.5),\n",
        "    #A.OpticalDistortion(p=0.5),\n",
        "    A.Normalize(mean=[0.5], std=[0.5]),\n",
        "    ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awaK8fHzWdJS"
      },
      "source": [
        "MAX_SEQ_LEN = 53\n",
        "trainset = IAMDataset(\n",
        "    \"train.txt\",\n",
        "    transform=train_transform,\n",
        "    max_sequence_len=MAX_SEQ_LEN,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT8dGvGP38b-"
      },
      "source": [
        "train_iter = iter(trainset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lNTbke04L3_"
      },
      "source": [
        "for _ in range(10):\n",
        "  data = next(train_iter)\n",
        "  print(data['label_str'])\n",
        "  plt.imshow(data['image'], cmap='gray')\n",
        "  plt.show()\n",
        "  statX(data['image'])\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ1-LAU54ow3"
      },
      "source": [
        "len(trainset.vocab.alphabet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL5bPyah_lBp"
      },
      "source": [
        "trainset.vocab.alphabet.index(\"<SEP>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3KVZ_0xYdrU"
      },
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=[0.5], std=[0.5]),\n",
        "    ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvrwffjwYjJt"
      },
      "source": [
        "testset = IAMDataset(\n",
        "    \"test.txt\",\n",
        "    transform=val_transform,\n",
        "    max_sequence_len=MAX_SEQ_LEN,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfso8DUlY7dj"
      },
      "source": [
        "if do_data_analysis:\n",
        "  max_h=0\n",
        "  max_w=0\n",
        "  max_l=0\n",
        "  vocab = set()\n",
        "  count=0\n",
        "  for data in trainset:\n",
        "    img = data['image']\n",
        "    max_w = max(max_w, img.shape[1])\n",
        "    max_h = max(max_h, img.shape[0])\n",
        "    text = data['label_str']\n",
        "    max_l = max(max_l, len(text))\n",
        "    for c in text:\n",
        "      if c not in vocab:\n",
        "        vocab.add(c)\n",
        "  print(f'Max height={max_h}, width={max_w}, label={max_l}')\n",
        "  vocab = list(vocab)\n",
        "  vocab.sort()\n",
        "  print(''.join(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V0TvsuBZTwM"
      },
      "source": [
        "if do_data_analysis:\n",
        "  max_h=0\n",
        "  max_w=0\n",
        "  max_l=0\n",
        "  vocab = set()\n",
        "  count=0\n",
        "  for data in testset:\n",
        "    img = data['image']\n",
        "    max_w = max(max_w, img.shape[1])\n",
        "    max_h = max(max_h, img.shape[0])\n",
        "    text = data['label_str']\n",
        "    max_l = max(max_l, len(text))\n",
        "    for c in text:\n",
        "      if c not in vocab:\n",
        "        vocab.add(c)\n",
        "  print(f'Max height={max_h}, width={max_w}, label={max_l}')\n",
        "  vocab = list(vocab)\n",
        "  vocab.sort()\n",
        "  print(''.join(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbI8ROd54eVp"
      },
      "source": [
        "def count_parameters(model):\n",
        "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return params / 1000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsh19K4j_Tr2"
      },
      "source": [
        "num_classes = len(trainset.vocab.alphabet)\n",
        "blank_label = trainset.vocab.alphabet.index(\"<SEP>\")\n",
        "gru_input_size = 256\n",
        "gru_hidden_size = 256\n",
        "gru_num_layers = 4\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.conv = EfficientNet.from_pretrained('efficientnet-b0', in_channels=1)\n",
        "        self.gru = nn.GRU(gru_input_size, gru_hidden_size, gru_num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(gru_hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        out = self.conv.extract_features(x)\n",
        "        #print(f\"After CNN: {out.shape}\")\n",
        "        out = out.reshape(batch_size, -1, gru_input_size)\n",
        "        #print(f\"After reshape: {out.shape}\")\n",
        "        out, _ = self.gru(out)\n",
        "        #print(f\"After GRU: {out.shape}\")\n",
        "        out = torch.stack([F.log_softmax(self.fc(out[i]), dim=-1) for i in range(out.shape[0])])\n",
        "        return out.permute(1, 0 , 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1SA8knadMQ5"
      },
      "source": [
        "model = CRNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HwYDDJcdO8R"
      },
      "source": [
        "x = torch.randn(1,1,*IMG_SIZE)\n",
        "out = model(x)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxtqVyntdWvw"
      },
      "source": [
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFQML8jmdY4w"
      },
      "source": [
        "# load pre-trained weights\n",
        "if load_weights and os.path.exists(weigths_file):\n",
        "  model.load_state_dict(torch.load(weigths_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpnQlnsndGKa"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, collate_fn=base_collator_with_padding(\"label_indices\"), shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, collate_fn=base_collator_with_padding(\"label_indices\"), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-JU3ooPdtO3"
      },
      "source": [
        "criterion = nn.CTCLoss(blank=blank_label, reduction='mean', zero_infinity=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=3, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jZ6h6AaeXK6"
      },
      "source": [
        "print(f\"Model has {count_parameters(model):.2f}M parameters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOhIlw8FeZtR"
      },
      "source": [
        "def create_logit_lengths(batch_size, sequence_length):\n",
        "    return torch.full((batch_size,), sequence_length, dtype=torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fijAW7Jece6"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPHeIypuehfA"
      },
      "source": [
        "epochs=200\n",
        "focal_loss = False\n",
        "# ================================================ TRAINING MODEL ======================================================\n",
        "early_stopping =  EarlyStopping(verbose=True, path=weigths_file)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # ============================================ TRAINING ============================================================\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_ed = 0\n",
        "    train_cc = 0\n",
        "    for data in tqdm(train_loader,\n",
        "                     position=0, leave=True,\n",
        "                     file=sys.stdout, bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.GREEN, Fore.RESET)):\n",
        "        tensor_keys = [\"image\", \"label_indices\", \"label_length\"]\n",
        "        images, label_indices, label_lengths = tuple(\n",
        "            data[k].cuda() for k in tensor_keys\n",
        "        )\n",
        "\n",
        "        b, h, w = images.shape\n",
        "        images = images.reshape(b, 1 , h, w)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred = model(images)\n",
        "\n",
        "        ll = create_logit_lengths(y_pred.shape[1], y_pred.shape[0]) \n",
        "        loss = criterion(y_pred, label_indices, ll, label_lengths)\n",
        "\n",
        "        # focal loss\n",
        "        if focal_loss:\n",
        "          p = torch.exp(-loss)\n",
        "          # alpha=0.25, gamma=0.5\n",
        "          loss = 0.25 * (1 - p)**0.5 * loss\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        preds = torch.argmax(y_pred, dim=-1)\n",
        "        preds_size = torch.full(\n",
        "            (preds.shape[1],), fill_value=preds.shape[0], dtype=torch.int64\n",
        "        )\n",
        "\n",
        "        flattened_preds = preds.transpose(1, 0).contiguous().view(-1)\n",
        "        preds_decode = trainset.vocab.decode(flattened_preds.data, preds_size.data)\n",
        "\n",
        "        labels = data['label_str']\n",
        "\n",
        "        for label, pred in zip(labels, preds_decode):\n",
        "          train_ed += editdistance.eval(pred, label)\n",
        "          train_cc += len(label)\n",
        "          train_total += 1\n",
        "\n",
        "    print(f'TRAINING. editdistance: {train_ed}/{train_total}={train_ed/train_total:.3f}')\n",
        "    print(f\"[{epoch}] Train loss: {train_loss:.2f}, avg. char count={train_cc/train_total:.3f}\")\n",
        "\n",
        "    # ========================= VALIDATION =====================================\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_ed = 0\n",
        "    val_cc = 0\n",
        "    TO_DISPLAY = 10\n",
        "    learnt = set()\n",
        "    best_outputs = dict()\n",
        "    for k, data in enumerate(test_loader):\n",
        "        tensor_keys = [\"image\", \"label_indices\", \"label_length\"]\n",
        "        images, label_indices, label_lengths = tuple(\n",
        "            data[k].cuda() for k in tensor_keys\n",
        "        )\n",
        "\n",
        "        b, h, w = images.shape\n",
        "        images = images.reshape(b, 1 , h, w)\n",
        "        \n",
        "        with torch.set_grad_enabled(False):\n",
        "            y_pred = model(images)\n",
        "        \n",
        "        y_pred = model(images)\n",
        "\n",
        "        l = create_logit_lengths(y_pred.shape[1], y_pred.shape[0]) \n",
        "        loss = criterion(y_pred, label_indices, l, label_lengths)\n",
        "\n",
        "        # focal loss\n",
        "        if focal_loss:\n",
        "          p = torch.exp(-loss)\n",
        "          # alpha=0.25, gamma=0.5\n",
        "          loss = 0.25 * (1 - p)**0.5 * loss\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(y_pred, dim=-1)\n",
        "        preds_size = torch.full(\n",
        "            (preds.shape[1],), fill_value=preds.shape[0], dtype=torch.int64\n",
        "        )\n",
        "\n",
        "        flattened_preds = preds.transpose(1, 0).contiguous().view(-1)\n",
        "        preds_decode = trainset.vocab.decode(flattened_preds.data, preds_size.data)\n",
        "\n",
        "        labels = data['label_str']\n",
        "\n",
        "        for label, pred in zip(labels, preds_decode):\n",
        "          for c in pred:\n",
        "            learnt.add(c)\n",
        "          ed = editdistance.eval(pred, label)\n",
        "          val_ed += ed\n",
        "          best_outputs[k] = (ed/len(label), label, pred)\n",
        "          val_cc += len(label)\n",
        "          val_total += 1\n",
        "\n",
        "    # print learnt characters\n",
        "    print(f'Learnt {len(learnt)} characters: {learnt}')\n",
        "\n",
        "    # print best scored sentences\n",
        "    best_scored = [(v[1], v[2]) for k, v in sorted(best_outputs.items(), key=lambda item: item[1][0])]\n",
        "    for k in range(TO_DISPLAY):\n",
        "      print(f\"Output={best_scored[k][1]}\\t Label={best_scored[k][0]}\")\n",
        "\n",
        "    print(f'TESTING editdistance:  {val_ed}/{val_total} = {val_ed/val_total:.3f}')\n",
        "    print(f\"[{epoch}] Test loss: {val_loss:.2f}, avg char count: {val_cc/val_total:.3f}\")\n",
        "\n",
        "    # step scheduler using validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # early stopping\n",
        "    early_stopping(val_loss, model)\n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "      print(f\"Early stopping, loading best weight.. , val loss={-early_stopping.best_score:.2f}\")\n",
        "      model.load_state_dict(torch.load(early_stopping.path))\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFbhB1Uf4avX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvsmVEGd4XsT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTet5QBc4WXD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}